{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "436dbaf5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-07T17:33:25.682707Z",
     "iopub.status.busy": "2025-12-07T17:33:25.682420Z",
     "iopub.status.idle": "2025-12-07T17:35:21.756727Z",
     "shell.execute_reply": "2025-12-07T17:35:21.755964Z"
    },
    "id": "aSifJCI--9Xn",
    "outputId": "70779f27-92b7-40bc-f737-9a15de0343bd",
    "papermill": {
     "duration": 116.092128,
     "end_time": "2025-12-07T17:35:21.758194",
     "exception": false,
     "start_time": "2025-12-07T17:33:25.666066",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting protobuf==4.25.*\r\n",
      "  Downloading protobuf-4.25.8-cp37-abi3-manylinux2014_x86_64.whl.metadata (541 bytes)\r\n",
      "Downloading protobuf-4.25.8-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.9/294.9 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: protobuf\r\n",
      "  Attempting uninstall: protobuf\r\n",
      "    Found existing installation: protobuf 6.33.0\r\n",
      "    Uninstalling protobuf-6.33.0:\r\n",
      "      Successfully uninstalled protobuf-6.33.0\r\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "bigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\r\n",
      "opentelemetry-proto 1.37.0 requires protobuf<7.0,>=5.0, but you have protobuf 4.25.8 which is incompatible.\r\n",
      "a2a-sdk 0.3.10 requires protobuf>=5.29.5, but you have protobuf 4.25.8 which is incompatible.\r\n",
      "ray 2.51.1 requires click!=8.3.0,>=7.0, but you have click 8.3.0 which is incompatible.\r\n",
      "bigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\r\n",
      "pydrive2 1.21.3 requires cryptography<44, but you have cryptography 46.0.3 which is incompatible.\r\n",
      "pydrive2 1.21.3 requires pyOpenSSL<=24.2.1,>=19.1.0, but you have pyopenssl 25.3.0 which is incompatible.\r\n",
      "ydf 0.13.0 requires protobuf<7.0.0,>=5.29.1, but you have protobuf 4.25.8 which is incompatible.\r\n",
      "grpcio-status 1.71.2 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 4.25.8 which is incompatible.\r\n",
      "gcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2025.10.0 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0mSuccessfully installed protobuf-4.25.8\r\n",
      "Collecting bitsandbytes\r\n",
      "  Downloading bitsandbytes-0.48.2-py3-none-manylinux_2_24_x86_64.whl.metadata (10 kB)\r\n",
      "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.9.0)\r\n",
      "Collecting accelerate\r\n",
      "  Downloading accelerate-1.12.0-py3-none-any.whl.metadata (19 kB)\r\n",
      "Requirement already satisfied: torch<3,>=2.3 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.6.0+cu124)\r\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (1.26.4)\r\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (25.0)\r\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (7.1.3)\r\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from accelerate) (6.0.3)\r\n",
      "Requirement already satisfied: huggingface_hub>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (0.36.0)\r\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from accelerate) (0.5.3)\r\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.21.0->accelerate) (3.20.0)\r\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.21.0->accelerate) (2025.10.0)\r\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.21.0->accelerate) (2.32.5)\r\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.21.0->accelerate) (4.67.1)\r\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.21.0->accelerate) (4.15.0)\r\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.21.0->accelerate) (1.2.0)\r\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->bitsandbytes) (1.3.8)\r\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->bitsandbytes) (1.2.4)\r\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->bitsandbytes) (0.1.1)\r\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->bitsandbytes) (2025.3.0)\r\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->bitsandbytes) (2022.3.0)\r\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->bitsandbytes) (2.4.1)\r\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.5)\r\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.1.6)\r\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch<3,>=2.3->bitsandbytes)\r\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\r\n",
      "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch<3,>=2.3->bitsandbytes)\r\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\r\n",
      "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch<3,>=2.3->bitsandbytes)\r\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\r\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch<3,>=2.3->bitsandbytes)\r\n",
      "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\r\n",
      "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch<3,>=2.3->bitsandbytes)\r\n",
      "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\r\n",
      "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch<3,>=2.3->bitsandbytes)\r\n",
      "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\r\n",
      "Collecting nvidia-curand-cu12==10.3.5.147 (from torch<3,>=2.3->bitsandbytes)\r\n",
      "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\r\n",
      "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch<3,>=2.3->bitsandbytes)\r\n",
      "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\r\n",
      "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch<3,>=2.3->bitsandbytes)\r\n",
      "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\r\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (0.6.2)\r\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (2.21.5)\r\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.4.127)\r\n",
      "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch<3,>=2.3->bitsandbytes)\r\n",
      "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\r\n",
      "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.2.0)\r\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (1.13.1)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<3,>=2.3->bitsandbytes) (1.3.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<3,>=2.3->bitsandbytes) (3.0.3)\r\n",
      "Requirement already satisfied: onemkl-license==2025.3.0 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->bitsandbytes) (2025.3.0)\r\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->bitsandbytes) (2024.2.0)\r\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->bitsandbytes) (2022.3.0)\r\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->bitsandbytes) (1.4.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->bitsandbytes) (2024.2.0)\r\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub>=0.21.0->accelerate) (3.4.4)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub>=0.21.0->accelerate) (3.11)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub>=0.21.0->accelerate) (2.5.0)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub>=0.21.0->accelerate) (2025.10.5)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->bitsandbytes) (2024.2.0)\r\n",
      "Downloading bitsandbytes-0.48.2-py3-none-manylinux_2_24_x86_64.whl (59.4 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.4/59.4 MB\u001b[0m \u001b[31m32.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading accelerate-1.12.0-py3-none-any.whl (380 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m380.9/380.9 kB\u001b[0m \u001b[31m25.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m116.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m83.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m45.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m22.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m54.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, bitsandbytes, accelerate\r\n",
      "  Attempting uninstall: nvidia-nvjitlink-cu12\r\n",
      "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\r\n",
      "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\r\n",
      "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\r\n",
      "  Attempting uninstall: nvidia-curand-cu12\r\n",
      "    Found existing installation: nvidia-curand-cu12 10.3.6.82\r\n",
      "    Uninstalling nvidia-curand-cu12-10.3.6.82:\r\n",
      "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\r\n",
      "  Attempting uninstall: nvidia-cufft-cu12\r\n",
      "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\r\n",
      "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\r\n",
      "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\r\n",
      "  Attempting uninstall: nvidia-cuda-runtime-cu12\r\n",
      "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\r\n",
      "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\r\n",
      "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\r\n",
      "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\r\n",
      "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\r\n",
      "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\r\n",
      "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\r\n",
      "  Attempting uninstall: nvidia-cuda-cupti-cu12\r\n",
      "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\r\n",
      "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\r\n",
      "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\r\n",
      "  Attempting uninstall: nvidia-cublas-cu12\r\n",
      "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\r\n",
      "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\r\n",
      "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\r\n",
      "  Attempting uninstall: nvidia-cusparse-cu12\r\n",
      "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\r\n",
      "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\r\n",
      "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\r\n",
      "  Attempting uninstall: nvidia-cudnn-cu12\r\n",
      "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\r\n",
      "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\r\n",
      "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\r\n",
      "  Attempting uninstall: nvidia-cusolver-cu12\r\n",
      "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\r\n",
      "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\r\n",
      "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\r\n",
      "  Attempting uninstall: accelerate\r\n",
      "    Found existing installation: accelerate 1.9.0\r\n",
      "    Uninstalling accelerate-1.9.0:\r\n",
      "      Successfully uninstalled accelerate-1.9.0\r\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "libcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\r\n",
      "pylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\r\n",
      "pylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0mSuccessfully installed accelerate-1.12.0 bitsandbytes-0.48.2 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\r\n",
      "Requirement already satisfied: polars in /usr/local/lib/python3.11/dist-packages (1.25.0)\r\n",
      "Collecting polars\r\n",
      "  Downloading polars-1.35.2-py3-none-any.whl.metadata (10 kB)\r\n",
      "Collecting polars-runtime-32==1.35.2 (from polars)\r\n",
      "  Downloading polars_runtime_32-1.35.2-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.5 kB)\r\n",
      "Downloading polars-1.35.2-py3-none-any.whl (783 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m783.6/783.6 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading polars_runtime_32-1.35.2-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (41.3 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.3/41.3 MB\u001b[0m \u001b[31m42.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: polars-runtime-32, polars\r\n",
      "  Attempting uninstall: polars\r\n",
      "    Found existing installation: polars 1.25.0\r\n",
      "    Uninstalling polars-1.25.0:\r\n",
      "      Successfully uninstalled polars-1.25.0\r\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "cudf-polars-cu12 25.6.0 requires polars<1.29,>=1.25, but you have polars 1.35.2 which is incompatible.\r\n",
      "cudf-polars-cu12 25.6.0 requires pylibcudf-cu12==25.6.*, but you have pylibcudf-cu12 25.2.2 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0mSuccessfully installed polars-1.35.2 polars-runtime-32-1.35.2\r\n",
      "Requirement already satisfied: scikit-multilearn in /usr/local/lib/python3.11/dist-packages (0.2.0)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade protobuf==4.25.*\n",
    "!pip install -U bitsandbytes accelerate\n",
    "!pip install --upgrade polars\n",
    "!pip install scikit-multilearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5c79aa0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-07T17:35:21.846813Z",
     "iopub.status.busy": "2025-12-07T17:35:21.846123Z",
     "iopub.status.idle": "2025-12-07T17:35:39.316142Z",
     "shell.execute_reply": "2025-12-07T17:35:39.315244Z"
    },
    "id": "jKV00lxQ-9Xo",
    "papermill": {
     "duration": 17.515178,
     "end_time": "2025-12-07T17:35:39.317616",
     "exception": false,
     "start_time": "2025-12-07T17:35:21.802438",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import os\n",
    "import re\n",
    "import torch\n",
    "import json \n",
    "import gc\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, mean_absolute_error, confusion_matrix\n",
    "from skmultilearn.model_selection import IterativeStratification\n",
    "from scipy.stats import pearsonr\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "\n",
    "torch._dynamo.config.cache_size_limit = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02323cc6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-07T17:35:39.405704Z",
     "iopub.status.busy": "2025-12-07T17:35:39.405315Z",
     "iopub.status.idle": "2025-12-07T17:38:01.856916Z",
     "shell.execute_reply": "2025-12-07T17:38:01.856275Z"
    },
    "id": "RWfzsaNl-9Xo",
    "outputId": "f8b7cae9-c365-47a1-8c45-dfcdab75cb6d",
    "papermill": {
     "duration": 142.496748,
     "end_time": "2025-12-07T17:38:01.858188",
     "exception": false,
     "start_time": "2025-12-07T17:35:39.361440",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n",
      "2025-12-07 17:35:48.931732: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1765128949.296166      20 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1765128949.409046      20 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71223b6d575040f7882b828a534c28ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ==========================================\n",
    "# 1. SETUP & MODEL LOADING (QUANTIZED)\n",
    "# ==========================================\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model_name = \"/kaggle/input/qwen-3/transformers/8b/1\" \n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ").eval()\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11991a7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-07T17:38:01.947380Z",
     "iopub.status.busy": "2025-12-07T17:38:01.946365Z",
     "iopub.status.idle": "2025-12-07T17:38:04.635923Z",
     "shell.execute_reply": "2025-12-07T17:38:04.635347Z"
    },
    "id": "Sq5ZSCc6-9Xp",
    "outputId": "7aa122b9-5250-4f1a-e896-24e42e3c1e1c",
    "papermill": {
     "duration": 2.734689,
     "end_time": "2025-12-07T17:38:04.637113",
     "exception": false,
     "start_time": "2025-12-07T17:38:01.902424",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Big5 Sample shape: (800, 7)\n",
      "MBTI Sample shape: (800, 2)\n",
      "Personae loaded rows: 145\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "152"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ==========================================\n",
    "# 2. DATA LOADING\n",
    "# ==========================================\n",
    "\n",
    "# --- 1. Essay Big5 (OCEAN) ---\n",
    "try:\n",
    "    essay_paths = [\n",
    "        \"/kaggle/input/essays-big5/essays-big5/test-00000-of-00001.parquet\",\n",
    "        \"/kaggle/input/essays-big5/essays-big5/train-00000-of-00001.parquet\",\n",
    "        \"/kaggle/input/essays-big5/essays-big5/validation-00000-of-00001.parquet\"\n",
    "    ]\n",
    "    # Фильтруем существующие пути\n",
    "    valid_paths = [p for p in essay_paths if os.path.exists(p)]\n",
    "    \n",
    "    if valid_paths:\n",
    "        essay_big5_df = pl.concat([pl.read_parquet(p) for p in valid_paths])\n",
    "        essay_big5_df = essay_big5_df.drop(\"ptype\")\n",
    "        \n",
    "        # Cast labels\n",
    "        essay_big5_df = essay_big5_df.with_columns([\n",
    "            pl.col(\"O\").cast(pl.Int64), pl.col(\"C\").cast(pl.Int64), pl.col(\"E\").cast(pl.Int64),\n",
    "            pl.col(\"A\").cast(pl.Int64), pl.col(\"N\").cast(pl.Int64),\n",
    "        ])\n",
    "\n",
    "        # Stratification logic\n",
    "        X = np.zeros((len(essay_big5_df), 1))\n",
    "        y = essay_big5_df[[\"O\", \"C\", \"E\", \"A\", \"N\"]].to_numpy()\n",
    "\n",
    "        stratifier = IterativeStratification(n_splits=2, order=1, sample_distribution_per_fold=[0.324, 0.676])\n",
    "        train_idx, sample_idx = next(stratifier.split(X, y))\n",
    "        \n",
    "        eb5_sample = essay_big5_df[sample_idx]\n",
    "        print(f\"Big5 Sample shape: {eb5_sample.shape}\")\n",
    "    else:\n",
    "        print(\"Warning: Big5 files not found.\")\n",
    "        eb5_sample = pl.DataFrame()\n",
    "except Exception as e:\n",
    "    print(f\"Error loading Big5: {e}\")\n",
    "    eb5_sample = pl.DataFrame()\n",
    "\n",
    "\n",
    "# --- 2. MBTI ---\n",
    "try:\n",
    "    mbti_path = \"/kaggle/input/mbti-type/mbti_1.csv\"\n",
    "    if os.path.exists(mbti_path):\n",
    "        mbti_df = pl.read_csv(mbti_path)\n",
    "        y_mbti = mbti_df[\"type\"].to_list()\n",
    "        \n",
    "        mbti_train_pd, mbti_sample_pd = train_test_split(\n",
    "            mbti_df.to_pandas(),\n",
    "            test_size=0.0922,\n",
    "            stratify=y_mbti,\n",
    "            random_state=42\n",
    "        )\n",
    "        mbti_sample = pl.from_pandas(mbti_sample_pd)\n",
    "        print(f\"MBTI Sample shape: {mbti_sample.shape}\")\n",
    "    else:\n",
    "        print(\"Warning: MBTI file not found.\")\n",
    "        mbti_sample = pl.DataFrame()\n",
    "except Exception as e:\n",
    "    print(f\"Error loading MBTI: {e}\")\n",
    "    mbti_sample = pl.DataFrame()\n",
    "\n",
    "\n",
    "# --- 3. Personae ---\n",
    "records = []\n",
    "folder = \"/kaggle/input/personae-corpus/PersonaeCorpus/data\"\n",
    "if os.path.exists(folder):\n",
    "    for f in os.listdir(folder):\n",
    "        parts = f.split(\".\")\n",
    "        if len(parts) >= 3:\n",
    "            try:\n",
    "                with open(os.path.join(folder, f), \"r\", encoding=\"utf-8\", errors='ignore') as ft:\n",
    "                    records.append({\n",
    "                        \"id\": parts[0], \"gender\": parts[1], \"mbti\": parts[2], \"text\": ft.read()\n",
    "                    })\n",
    "            except Exception:\n",
    "                continue\n",
    "    personae_df = pl.DataFrame(records)\n",
    "    print(f\"Personae loaded rows: {len(personae_df)}\")\n",
    "else:\n",
    "    print(\"Warning: Personae folder not found.\")\n",
    "    personae_df = pl.DataFrame({\"id\": [], \"gender\": [], \"mbti\": [], \"text\": []})\n",
    "\n",
    "# Очистка памяти (удаляем только то, что загружали)\n",
    "# essay_big5_df удаляем, т.к. создали eb5_sample\n",
    "if 'essay_big5_df' in locals(): del essay_big5_df\n",
    "if 'mbti_df' in locals(): del mbti_df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "77d0f1b9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-07T17:38:04.726385Z",
     "iopub.status.busy": "2025-12-07T17:38:04.725909Z",
     "iopub.status.idle": "2025-12-07T17:38:04.740213Z",
     "shell.execute_reply": "2025-12-07T17:38:04.739687Z"
    },
    "id": "gCgmOcBv-9Xq",
    "papermill": {
     "duration": 0.05961,
     "end_time": "2025-12-07T17:38:04.741339",
     "exception": false,
     "start_time": "2025-12-07T17:38:04.681729",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 3. INFERENCE ENGINE\n",
    "# ==========================================\n",
    "def run_inference(df, prompt_fn, parse_fn, batch_size=8, max_rows=None):\n",
    "    if df.is_empty():\n",
    "        print(\"Dataframe is empty, skipping inference.\")\n",
    "        return pl.DataFrame()\n",
    "\n",
    "    if max_rows:\n",
    "        df = df.head(max_rows)\n",
    "\n",
    "    results = []\n",
    "    prompts = []\n",
    "\n",
    "    print(f\"Building prompts for {len(df)} rows...\")\n",
    "    for row in df.iter_rows(named=True):\n",
    "        user_content = prompt_fn(row)\n",
    "        messages = [{\"role\": \"user\", \"content\": user_content}]\n",
    "        rendered = tokenizer.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)\n",
    "        prompts.append(rendered)\n",
    "\n",
    "    print(\"Starting generation...\")\n",
    "    for i in range(0, len(prompts), batch_size):\n",
    "        batch_prompts = prompts[i:i+batch_size]\n",
    "        batch_rows = df[i:i+batch_size].iter_rows(named=True)\n",
    "\n",
    "        inputs = tokenizer(\n",
    "            batch_prompts, return_tensors=\"pt\", padding=True, truncation=True, max_length=2048\n",
    "        ).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs, max_new_tokens=60, pad_token_id=tokenizer.eos_token_id, do_sample=False, use_cache=True\n",
    "            )\n",
    "\n",
    "        input_len = inputs.input_ids.shape[1]\n",
    "        generated = outputs[:, input_len:]\n",
    "        replies = tokenizer.batch_decode(generated, skip_special_tokens=True)\n",
    "\n",
    "        for row, reply in zip(batch_rows, replies):\n",
    "            parsed_data = parse_fn(reply, row)\n",
    "            results.append(parsed_data)\n",
    "\n",
    "    return pl.DataFrame(results)\n",
    "\n",
    "# Глобальный список для метрик\n",
    "all_metrics_data = []\n",
    "\n",
    "def calculate_metrics(y_true, y_pred, task_name, dataset_name):\n",
    "    valid_data = [\n",
    "        (t, p) for t, p in zip(y_true, y_pred) \n",
    "        if p is not None and p != \"XXXX\" and p != -1 and p != \"unknown\"\n",
    "    ]\n",
    "\n",
    "    if not valid_data:\n",
    "        print(f\"[{dataset_name} - {task_name}] No valid predictions.\")\n",
    "        return\n",
    "\n",
    "    y_true_clean = [x[0] for x in valid_data]\n",
    "    y_pred_clean = [x[1] for x in valid_data]\n",
    "    n = len(y_true_clean)\n",
    "\n",
    "    print(f\"\\n--- Metrics for {dataset_name}: {task_name} (N={n}) ---\")\n",
    "\n",
    "    # Проверяем MBTI (строка 4 буквы)\n",
    "    is_mbti_task = isinstance(y_true_clean[0], str) and len(y_true_clean[0]) == 4 and any(c in \"IE\" for c in y_true_clean[0])\n",
    "\n",
    "    if is_mbti_task:\n",
    "        acc = accuracy_score(y_true_clean, y_pred_clean)\n",
    "        _, _, f1_macro, _ = precision_recall_fscore_support(y_true_clean, y_pred_clean, average='macro', zero_division=0)\n",
    "\n",
    "        print(f\"Exact Match Accuracy: {acc:.2%}\")\n",
    "        print(f\"Macro F1-Score:       {f1_macro:.2%}\")\n",
    "\n",
    "        axes = [\"(I)E\", \"(N)S\", \"(T)F\", \"(J)P\"]\n",
    "        axis_scores = []\n",
    "        total_letters_correct = 0\n",
    "        \n",
    "        for i in range(4):\n",
    "            correct_count = sum(1 for t, p in zip(y_true_clean, y_pred_clean) if t[i] == p[i])\n",
    "            axis_acc = correct_count / n\n",
    "            axis_scores.append(axis_acc)\n",
    "            total_letters_correct += correct_count\n",
    "            print(f\"Axis {axes[i]} Accuracy:       {axis_acc:.2%}\")\n",
    "\n",
    "        avg_letters = total_letters_correct / n\n",
    "        print(f\"Avg Letters Correct:  {avg_letters:.2f} / 4.00\")\n",
    "\n",
    "        all_metrics_data.append({\n",
    "            \"Dataset\": dataset_name, \"Task\": task_name,\n",
    "            \"Accuracy\": acc, \"F1_Macro\": f1_macro,\n",
    "            \"Axis_IE\": axis_scores[0], \"Axis_NS\": axis_scores[1],\n",
    "            \"Axis_TF\": axis_scores[2], \"Axis_JP\": axis_scores[3],\n",
    "            \"Avg_Letters\": avg_letters\n",
    "        })\n",
    "    else:\n",
    "        # Big 5 / Gender\n",
    "        acc = accuracy_score(y_true_clean, y_pred_clean)\n",
    "        avg_method = 'binary' if len(set(y_true_clean)) <= 2 and isinstance(y_true_clean[0], int) else 'weighted'\n",
    "        prec, rec, f1, _ = precision_recall_fscore_support(y_true_clean, y_pred_clean, average=avg_method, zero_division=0)\n",
    "        \n",
    "        print(f\"Accuracy:  {acc:.2%}\")\n",
    "        print(f\"F1-Score:  {f1:.2%} ({avg_method})\")\n",
    "\n",
    "        mae, pearson_corr = None, None\n",
    "        \n",
    "        if isinstance(y_true_clean[0], (int, float, np.number)):\n",
    "            mae = mean_absolute_error(y_true_clean, y_pred_clean)\n",
    "            if len(set(y_true_clean)) > 1 and len(set(y_pred_clean)) > 1:\n",
    "                pearson_corr, _ = pearsonr(y_true_clean, y_pred_clean)\n",
    "                print(f\"Pearson Corr: {pearson_corr:.4f}\")\n",
    "            print(f\"MAE:       {mae:.4f}\")\n",
    "\n",
    "        all_metrics_data.append({\n",
    "            \"Dataset\": dataset_name, \"Task\": task_name,\n",
    "            \"Accuracy\": acc, \"F1\": f1, \"Precision\": prec, \"Recall\": rec,\n",
    "            \"MAE\": mae, \"Pearson\": pearson_corr\n",
    "        })\n",
    "\n",
    "    print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6e6af8d8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-07T17:38:04.902127Z",
     "iopub.status.busy": "2025-12-07T17:38:04.901858Z",
     "iopub.status.idle": "2025-12-07T17:38:04.915348Z",
     "shell.execute_reply": "2025-12-07T17:38:04.914779Z"
    },
    "id": "Fj99-2lT-9Xr",
    "outputId": "8175db5f-f3b9-4426-f0c9-b26d5a738fe4",
    "papermill": {
     "duration": 0.131725,
     "end_time": "2025-12-07T17:38:04.916384",
     "exception": false,
     "start_time": "2025-12-07T17:38:04.784659",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 4. PROMPTS & PARSERS DEFINITIONS\n",
    "# ==========================================\n",
    "\n",
    "# --- Big5 ---\n",
    "def big5_prompt(row):\n",
    "    return f\"\"\"Analyze the text and predict the Big Five personality traits.\n",
    "Traits: O (Openness), C (Conscientiousness), E (Extraversion), A (Agreeableness), N (Neuroticism).\n",
    "Return a valid JSON object with keys \"O\", \"C\", \"E\", \"A\", \"N\" and values 0 (Low) or 1 (High).\n",
    "Example: {{ \"O\": 1, \"C\": 0, \"E\": 1, \"A\": 1, \"N\": 0 }}\n",
    "Text: {row['text'][:2000]}\"\"\"\n",
    "\n",
    "def big5_parse(reply, row):\n",
    "    preds = {\"O\": -1, \"C\": -1, \"E\": -1, \"A\": -1, \"N\": -1}\n",
    "    try:\n",
    "        clean_json = re.sub(r\"```json|```\", \"\", reply).strip()\n",
    "        match_json = re.search(r\"\\{.*\\}\", clean_json, re.DOTALL)\n",
    "        if match_json:\n",
    "            data = json.loads(match_json.group(0))\n",
    "            for key in preds.keys():\n",
    "                if key in data: preds[key] = int(data[key])\n",
    "            return {\n",
    "                \"O_true\": int(row[\"O\"]), \"C_true\": int(row[\"C\"]), \"E_true\": int(row[\"E\"]),\n",
    "                \"A_true\": int(row[\"A\"]), \"N_true\": int(row[\"N\"]),\n",
    "                \"O_pred\": preds[\"O\"], \"C_pred\": preds[\"C\"], \"E_pred\": preds[\"E\"],\n",
    "                \"A_pred\": preds[\"A\"], \"N_pred\": preds[\"N\"],\n",
    "            }\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # Fallback regex\n",
    "    pattern = re.compile(r\"([OCEAN])[a-z]*\\s*[:=\\-]\\s*(0|1|high|low)\", re.IGNORECASE)\n",
    "    matches = pattern.findall(reply)\n",
    "    for char, val_str in matches:\n",
    "        val = 1 if ('1' in val_str or 'high' in val_str.lower()) else 0\n",
    "        preds[char.upper()] = val\n",
    "\n",
    "    for k in preds:\n",
    "        if preds[k] == -1: preds[k] = 0 # Default safe value\n",
    "\n",
    "    return {\n",
    "        \"O_true\": int(row[\"O\"]), \"C_true\": int(row[\"C\"]), \"E_true\": int(row[\"E\"]),\n",
    "        \"A_true\": int(row[\"A\"]), \"N_true\": int(row[\"N\"]),\n",
    "        \"O_pred\": preds[\"O\"], \"C_pred\": preds[\"C\"], \"E_pred\": preds[\"E\"],\n",
    "        \"A_pred\": preds[\"A\"], \"N_pred\": preds[\"N\"],\n",
    "    }\n",
    "\n",
    "# --- MBTI ---\n",
    "def mbti_prompt(row):\n",
    "    text_snippet = row['posts'][:3000]\n",
    "    return f\"\"\"Analyze the text data provided in the previous message to determine the author's MBTI type.\n",
    "\n",
    "Evaluate based on these 4 dimensions:\n",
    "1. (E) Extraversion vs (I) Introversion\n",
    "2. (S) Sensing vs (N) Intuition\n",
    "3. (T) Thinking vs (F) Feeling\n",
    "4. (J) Judging vs (P) Perceiving\n",
    "\n",
    "Return a JSON object: {{ \"predicted_type\": \"INTJ\" }}\n",
    "Text: \"{text_snippet}\" \"\"\"\n",
    "\n",
    "def mbti_parse(reply, row):\n",
    "    try:\n",
    "        clean_reply = reply.replace(\"```json\", \"\").replace(\"```\", \"\").strip()\n",
    "        json_match = re.search(r\"\\{.*\\}\", clean_reply, re.DOTALL)\n",
    "        if json_match:\n",
    "            data = json.loads(json_match.group(0))\n",
    "            pred = data.get(\"predicted_type\", \"XXXX\").upper()\n",
    "            return {\"type_true\": row[\"type\"], \"type_pred\": pred}\n",
    "        \n",
    "        strict_match = re.search(r\"Type:?\\s*\\*?\\*?([IE][NS][TF][JP])\", reply, re.IGNORECASE)\n",
    "        if strict_match:\n",
    "            return {\"type_true\": row[\"type\"], \"type_pred\": strict_match.group(1).upper()}\n",
    "            \n",
    "        simple_match = re.search(r\"\\b([IE][NS][TF][JP])\\b\", reply.upper())\n",
    "        if simple_match:\n",
    "             return {\"type_true\": row[\"type\"], \"type_pred\": simple_match.group(1)}\n",
    "    except:\n",
    "        pass\n",
    "    return {\"type_true\": row[\"type\"], \"type_pred\": \"XXXX\"}\n",
    "\n",
    "# --- Personae ---\n",
    "def personae_prompt(row):\n",
    "    return f\"\"\"Analyze the text to predict Gender and MBTI.\n",
    "Return JSON: {{ \"gender\": \"Female\", \"mbti\": \"INFP\" }}\n",
    "Text: {row['text'][:2000]}\"\"\"\n",
    "\n",
    "def personae_parse(reply, row):\n",
    "    pred_gender, pred_mbti = \"unknown\", \"XXXX\"\n",
    "    try:\n",
    "        clean_json = re.sub(r\"```json|```\", \"\", reply).strip()\n",
    "        match_json = re.search(r\"\\{.*\\}\", clean_json, re.DOTALL)\n",
    "        if match_json:\n",
    "            data = json.loads(match_json.group(0))\n",
    "            if \"gender\" in data:\n",
    "                g = str(data[\"gender\"]).strip().lower()\n",
    "                if \"fem\" in g: pred_gender = \"female\"\n",
    "                elif \"mal\" in g: pred_gender = \"male\"\n",
    "            if \"mbti\" in data:\n",
    "                m = str(data[\"mbti\"]).strip().upper()\n",
    "                if re.match(r\"^[IE][NS][TF][JP]$\", m): pred_mbti = m\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # Regex fallback\n",
    "    if pred_gender == \"unknown\":\n",
    "        g_match = re.search(r\"Gender\\s*[:=\\-]\\s*(Male|Female)\", reply, re.IGNORECASE)\n",
    "        if g_match: pred_gender = g_match.group(1).lower()\n",
    "\n",
    "    if pred_mbti == \"XXXX\":\n",
    "        m_match = re.search(r\"\\b([IE][NS][TF][JP])\\b\", reply.upper())\n",
    "        if m_match: pred_mbti = m_match.group(1)\n",
    "\n",
    "    return {\n",
    "        \"gender_true\": row[\"gender\"].lower().strip(), \"gender_pred\": pred_gender,\n",
    "        \"mbti_true\": row[\"mbti\"], \"mbti_pred\": pred_mbti\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a950fa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-07T17:38:05.004084Z",
     "iopub.status.busy": "2025-12-07T17:38:05.003869Z",
     "iopub.status.idle": "2025-12-07T18:50:23.994599Z",
     "shell.execute_reply": "2025-12-07T18:50:23.993779Z"
    },
    "id": "ieCx76g4-9Xr",
    "papermill": {
     "duration": 4339.091779,
     "end_time": "2025-12-07T18:50:24.051935",
     "exception": false,
     "start_time": "2025-12-07T17:38:04.960156",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== RUNNING BIG5 INFERENCE ===\n",
      "Building prompts for 800 rows...\n",
      "Starting generation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Metrics for Big5_Essays: Big5_O (N=800) ---\n",
      "Accuracy:  48.38%\n",
      "F1-Score:  9.23% (binary)\n",
      "Pearson Corr: -0.0127\n",
      "MAE:       0.5162\n",
      "------------------------------\n",
      "\n",
      "--- Metrics for Big5_Essays: Big5_C (N=800) ---\n",
      "Accuracy:  49.50%\n",
      "F1-Score:  2.42% (binary)\n",
      "Pearson Corr: 0.0236\n",
      "MAE:       0.5050\n",
      "------------------------------\n",
      "\n",
      "--- Metrics for Big5_Essays: Big5_E (N=800) ---\n",
      "Accuracy:  49.12%\n",
      "F1-Score:  4.68% (binary)\n",
      "Pearson Corr: 0.0529\n",
      "MAE:       0.5088\n",
      "------------------------------\n",
      "\n",
      "--- Metrics for Big5_Essays: Big5_A (N=800) ---\n",
      "Accuracy:  47.88%\n",
      "F1-Score:  7.54% (binary)\n",
      "Pearson Corr: 0.0373\n",
      "MAE:       0.5212\n",
      "------------------------------\n",
      "\n",
      "--- Metrics for Big5_Essays: Big5_N (N=800) ---\n",
      "Accuracy:  49.50%\n",
      "F1-Score:  9.42% (binary)\n",
      "Pearson Corr: -0.0260\n",
      "MAE:       0.5050\n",
      "------------------------------\n",
      "\n",
      "=== RUNNING MBTI INFERENCE ===\n",
      "Building prompts for 800 rows...\n",
      "Starting generation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Metrics for MBTI_Dataset: MBTI Type (N=138) ---\n",
      "Exact Match Accuracy: 41.30%\n",
      "Macro F1-Score:       26.36%\n",
      "Axis (I)E Accuracy:       85.51%\n",
      "Axis (N)S Accuracy:       84.78%\n",
      "Axis (T)F Accuracy:       71.74%\n",
      "Axis (J)P Accuracy:       67.39%\n",
      "Avg Letters Correct:  3.09 / 4.00\n",
      "------------------------------\n",
      "\n",
      "=== RUNNING PERSONAE INFERENCE ===\n",
      "Building prompts for 50 rows...\n",
      "Starting generation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Metrics for Personae: Gender (N=20) ---\n",
      "Accuracy:  60.00%\n",
      "F1-Score:  45.00% (weighted)\n",
      "------------------------------\n",
      "\n",
      "--- Metrics for Personae: MBTI (N=20) ---\n",
      "Exact Match Accuracy: 0.00%\n",
      "Macro F1-Score:       0.00%\n",
      "Axis (I)E Accuracy:       50.00%\n",
      "Axis (N)S Accuracy:       55.00%\n",
      "Axis (T)F Accuracy:       55.00%\n",
      "Axis (J)P Accuracy:       20.00%\n",
      "Avg Letters Correct:  1.80 / 4.00\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# 5. EXECUTION PIPELINE\n",
    "# ==========================================\n",
    "\n",
    "# 1. Run Big5\n",
    "print(\"\\n=== RUNNING BIG5 INFERENCE ===\")\n",
    "big5_res = run_inference(eb5_sample, big5_prompt, big5_parse)\n",
    "if not big5_res.is_empty():\n",
    "    big5_res.write_csv(\"results_essay_big5.csv\")\n",
    "    for trait in [\"O\", \"C\", \"E\", \"A\", \"N\"]:\n",
    "        calculate_metrics(big5_res[f\"{trait}_true\"].to_list(), big5_res[f\"{trait}_pred\"].to_list(), f\"Big5_{trait}\", \"Big5_Essays\")\n",
    "\n",
    "# 2. Run MBTI\n",
    "print(\"\\n=== RUNNING MBTI INFERENCE ===\")\n",
    "mbti_res = run_inference(mbti_sample, mbti_prompt, mbti_parse)\n",
    "if not mbti_res.is_empty():\n",
    "    mbti_res.write_csv(\"results_mbti.csv\")\n",
    "    calculate_metrics(mbti_res[\"type_true\"].to_list(), mbti_res[\"type_pred\"].to_list(), \"MBTI Type\", \"MBTI_Dataset\")\n",
    "\n",
    "# 3. Run Personae\n",
    "print(\"\\n=== RUNNING PERSONAE INFERENCE ===\")\n",
    "personae_sample = personae_df.head(50) if len(personae_df) > 50 else personae_df\n",
    "personae_res = run_inference(personae_sample, personae_prompt, personae_parse)\n",
    "\n",
    "if not personae_res.is_empty():\n",
    "    personae_res.write_csv(\"results_personae.csv\")\n",
    "    calculate_metrics(personae_res[\"gender_true\"].to_list(), personae_res[\"gender_pred\"].to_list(), \"Gender\", \"Personae\")\n",
    "    calculate_metrics(personae_res[\"mbti_true\"].to_list(), personae_res[\"mbti_pred\"].to_list(), \"MBTI\", \"Personae\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dfa481bc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-07T18:50:24.158316Z",
     "iopub.status.busy": "2025-12-07T18:50:24.157703Z",
     "iopub.status.idle": "2025-12-07T18:50:24.165121Z",
     "shell.execute_reply": "2025-12-07T18:50:24.164461Z"
    },
    "id": "i96ndTp4-9Xr",
    "papermill": {
     "duration": 0.061509,
     "end_time": "2025-12-07T18:50:24.166162",
     "exception": false,
     "start_time": "2025-12-07T18:50:24.104653",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== SAVING METRICS SUMMARY ===\n",
      "shape: (8, 14)\n",
      "┌──────────────┬───────────┬──────────┬────────┬───┬─────────┬─────────┬─────────┬─────────────┐\n",
      "│ Dataset      ┆ Task      ┆ Accuracy ┆ F1     ┆ … ┆ Axis_NS ┆ Axis_TF ┆ Axis_JP ┆ Avg_Letters │\n",
      "│ ---          ┆ ---       ┆ ---      ┆ ---    ┆   ┆ ---     ┆ ---     ┆ ---     ┆ ---         │\n",
      "│ str          ┆ str       ┆ f64      ┆ f64    ┆   ┆ f64     ┆ f64     ┆ f64     ┆ f64         │\n",
      "╞══════════════╪═══════════╪══════════╪════════╪═══╪═════════╪═════════╪═════════╪═════════════╡\n",
      "│ Big5_Essays  ┆ Big5_O    ┆ 0.4838   ┆ 0.0923 ┆ … ┆ null    ┆ null    ┆ null    ┆ null        │\n",
      "│ Big5_Essays  ┆ Big5_C    ┆ 0.495    ┆ 0.0242 ┆ … ┆ null    ┆ null    ┆ null    ┆ null        │\n",
      "│ Big5_Essays  ┆ Big5_E    ┆ 0.4912   ┆ 0.0468 ┆ … ┆ null    ┆ null    ┆ null    ┆ null        │\n",
      "│ Big5_Essays  ┆ Big5_A    ┆ 0.4788   ┆ 0.0754 ┆ … ┆ null    ┆ null    ┆ null    ┆ null        │\n",
      "│ Big5_Essays  ┆ Big5_N    ┆ 0.495    ┆ 0.0942 ┆ … ┆ null    ┆ null    ┆ null    ┆ null        │\n",
      "│ MBTI_Dataset ┆ MBTI Type ┆ 0.413    ┆ null   ┆ … ┆ 0.8478  ┆ 0.7174  ┆ 0.6739  ┆ 3.0942      │\n",
      "│ Personae     ┆ Gender    ┆ 0.6      ┆ 0.45   ┆ … ┆ null    ┆ null    ┆ null    ┆ null        │\n",
      "│ Personae     ┆ MBTI      ┆ 0.0      ┆ null   ┆ … ┆ 0.55    ┆ 0.55    ┆ 0.2     ┆ 1.8         │\n",
      "└──────────────┴───────────┴──────────┴────────┴───┴─────────┴─────────┴─────────┴─────────────┘\n",
      "Metrics saved successfully.\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# 6. SAVE FINAL METRICS\n",
    "# ==========================================\n",
    "print(\"\\n=== SAVING METRICS SUMMARY ===\")\n",
    "if all_metrics_data:\n",
    "    metrics_df = pl.DataFrame(all_metrics_data)\n",
    "    \n",
    "    # Округляем числовые колонки\n",
    "    numeric_cols = [c for c in metrics_df.columns if metrics_df[c].dtype in [pl.Float64, pl.Float32]]\n",
    "    metrics_df = metrics_df.with_columns([pl.col(c).round(4) for c in numeric_cols])\n",
    "\n",
    "    print(metrics_df)\n",
    "    metrics_df.write_csv(\"all_metrics_summary.csv\")\n",
    "    print(\"Metrics saved successfully.\")\n",
    "else:\n",
    "    print(\"No metrics collected.\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 2637,
     "sourceId": 4381,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8759143,
     "sourceId": 13763790,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8759196,
     "sourceId": 13763859,
     "sourceType": "datasetVersion"
    },
    {
     "modelId": 322000,
     "modelInstanceId": 301517,
     "sourceId": 363139,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31193,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 4626.396348,
   "end_time": "2025-12-07T18:50:27.035447",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-12-07T17:33:20.639099",
   "version": "2.6.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "2c8f55a5967449058e65025e5edefb5d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "2f1e19831b3a4f1ca0b358b3143c3206": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "3099be12ff03457aa4d2692d60ff252d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "3752a453f35d4307a81f97cb49e5d818": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "3fdf157839324f22956f0f4e4acf2a89": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_5c81914df400446cb20449e667f8e71a",
       "max": 5,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_3752a453f35d4307a81f97cb49e5d818",
       "tabbable": null,
       "tooltip": null,
       "value": 5
      }
     },
     "5c81914df400446cb20449e667f8e71a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "71223b6d575040f7882b828a534c28ca": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_83871624fc024beab06e249ce89bfe95",
        "IPY_MODEL_3fdf157839324f22956f0f4e4acf2a89",
        "IPY_MODEL_e9003e672cd24e3d9f5f787865ab0109"
       ],
       "layout": "IPY_MODEL_3099be12ff03457aa4d2692d60ff252d",
       "tabbable": null,
       "tooltip": null
      }
     },
     "74ee482cec14458b800c959d2893cb2b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "83871624fc024beab06e249ce89bfe95": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_2c8f55a5967449058e65025e5edefb5d",
       "placeholder": "​",
       "style": "IPY_MODEL_fc56d2dac44c41f889c55d4005d18758",
       "tabbable": null,
       "tooltip": null,
       "value": "Loading checkpoint shards: 100%"
      }
     },
     "e9003e672cd24e3d9f5f787865ab0109": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_2f1e19831b3a4f1ca0b358b3143c3206",
       "placeholder": "​",
       "style": "IPY_MODEL_74ee482cec14458b800c959d2893cb2b",
       "tabbable": null,
       "tooltip": null,
       "value": " 5/5 [01:37&lt;00:00, 16.74s/it]"
      }
     },
     "fc56d2dac44c41f889c55d4005d18758": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
